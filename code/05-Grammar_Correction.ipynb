{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that does grammar correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'imports' from '/Users/konst/Documents/GitHub/Master_DS/data-wild-west/code/imports.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imports\n",
    "import imports as i\n",
    "import importlib\n",
    "importlib.reload(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = i.pd.read_csv('../data/processed_data/google_reviews.csv')\n",
    "text_rev = df['text'].tolist()\n",
    "\n",
    "corrected_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that does grammar correction\n",
    "\n",
    "sym_spell = i.SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = i.pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def grammar_corrector(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects spelling and grammar in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str or list): The input text to be corrected. It can be a single string or a list of strings.\n",
    "\n",
    "    Returns:\n",
    "        str or list: The corrected text, with spelling and grammar issues fixed.\n",
    "    \"\"\"\n",
    "    cleaned_text = []\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = [text]  # Convert a single string to a list of strings for consistency.\n",
    "\n",
    "    for line in text:\n",
    "        temp_line = []\n",
    "        words = line.split()\n",
    "        for _, word in enumerate(words):\n",
    "            # Check if the word contains a numeric character\n",
    "            has_numeric = any(char.isdigit() for char in word)\n",
    "            \n",
    "            if has_numeric:\n",
    "                # If the word contains a numeric character, keep the original word\n",
    "                corrected_word = word\n",
    "            else:\n",
    "                # If the word does not contain a numeric character, perform correction\n",
    "                corrected_word = sym_spell.lookup(word.lower(), i.Verbosity.CLOSEST, max_edit_distance=2)\n",
    "                corrected_word = corrected_word[0].term if corrected_word else corrected_word\n",
    "\n",
    "            # Append the punctuation back to the corrected word if the original word had it\n",
    "            if word[-1] in ['!', '?', '.']:\n",
    "                corrected_word += word[-1]\n",
    "\n",
    "            temp_line.append(corrected_word)\n",
    "\n",
    "            # Add space between words, except for the last word\n",
    "            if _ < len(words) - 1:\n",
    "                temp_line.append(' ')\n",
    "\n",
    "        cleaned_text.append(''.join(map(str, temp_line)))\n",
    "       \n",
    "\n",
    "    if isinstance(text, str):\n",
    "        return cleaned_text[0]  # Return the corrected string.\n",
    "    else:\n",
    "        return cleaned_text\n",
    "    \n",
    "for _ in text_rev:\n",
    "    corrected_text.append(grammar_corrector(_))\n",
    "df['corrected_review'] = corrected_text\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/konst/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/konst/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "### function to help the lemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\") \n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(postag):\n",
    "    if postag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif postag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif postag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif postag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "### lemmatizer   \n",
    "def lemmatize_sentencelist(sentencelist):\n",
    "    lemmatized_sentences = []\n",
    "    for s in sentencelist: \n",
    "        s = s.lower()\n",
    "        pos_s = nltk.pos_tag(s.split())\n",
    "        lemmatized_sentences.append(\" \".join([wnl.lemmatize(w[0], get_wordnet_pos(w[1])) for w in pos_s]))\n",
    "    return lemmatized_sentences\n",
    "\n",
    "df['lemmatized_review'] = df['corrected_review'].apply(lambda x: lemmatize_sentencelist(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown foxes are jumping over the lazy dogs or dog.\n",
      "Lemmatized: the quick brown fox be jump over the lazy dog or dog.\n",
      "---\n",
      "Original: She sells seashells by the seashore.\n",
      "Lemmatized: she sell seashell by the seashore.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "### Testing \n",
    "\n",
    "sentences = [\"The quick brown foxes are jumping over the lazy dogs or dog.\",\n",
    "             \"She sells seashells by the seashore.\"]\n",
    "\n",
    "lemmatized_sentences = lemmatize_sentencelist(sentences)\n",
    "\n",
    "# Print the original and lemmatized sentences\n",
    "for original, lemmatized in zip(sentences, lemmatized_sentences):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
