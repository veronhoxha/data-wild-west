{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that does grammar correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'imports' from '/Users/konst/Documents/GitHub/Master_DS/data-wild-west/code/imports.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imports\n",
    "import imports as i\n",
    "import utils as u\n",
    "from utils import lemmatize_with_postag\n",
    "import importlib\n",
    "importlib.reload(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = i.pd.read_csv('../data/processed_data/google_reviews.csv')\n",
    "text_rev = df['text'].tolist()\n",
    "\n",
    "corrected_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function that does grammar correction\n",
    "\n",
    "sym_spell = i.SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = i.pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def grammar_corrector(text:str) -> str:\n",
    "    \"\"\"\n",
    "    Corrects spelling and grammar in the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str or list): The input text to be corrected. It can be a single string or a list of strings.\n",
    "\n",
    "    Returns:\n",
    "        str or list: The corrected text, with spelling and grammar issues fixed.\n",
    "    \"\"\"\n",
    "    cleaned_text = []\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = [text]  # Convert a single string to a list of strings for consistency.\n",
    "\n",
    "    for line in text:\n",
    "        temp_line = []\n",
    "        words = line.split()\n",
    "        for _, word in enumerate(words):\n",
    "            # Check if the word contains a numeric character\n",
    "            has_numeric = any(char.isdigit() for char in word)\n",
    "            \n",
    "            if has_numeric:\n",
    "                # If the word contains a numeric character, keep the original word\n",
    "                corrected_word = word\n",
    "            else:\n",
    "                # If the word does not contain a numeric character, perform correction\n",
    "                corrected_word = sym_spell.lookup(word.lower(), i.Verbosity.CLOSEST, max_edit_distance=2)\n",
    "                corrected_word = corrected_word[0].term if corrected_word else corrected_word\n",
    "\n",
    "            # Append the punctuation back to the corrected word if the original word had it\n",
    "            if word[-1] in ['!', '?', '.']:\n",
    "                corrected_word += word[-1]\n",
    "\n",
    "            temp_line.append(corrected_word)\n",
    "\n",
    "            # Add space between words, except for the last word\n",
    "            if _ < len(words) - 1:\n",
    "                temp_line.append(' ')\n",
    "\n",
    "        cleaned_text.append(''.join(map(str, temp_line)))\n",
    "       \n",
    "\n",
    "    if isinstance(text, str):\n",
    "        return cleaned_text[0]  # Return the corrected string.\n",
    "    else:\n",
    "        return cleaned_text\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Users/konst/opt/anaconda3/envs/CBC/lib/python3.8/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /Users/konst/opt/anaconda3/envs/CBC/lib/python3.8/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: joblib in /Users/konst/opt/anaconda3/envs/CBC/lib/python3.8/site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: click in /Users/konst/opt/anaconda3/envs/CBC/lib/python3.8/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/konst/opt/anaconda3/envs/CBC/lib/python3.8/site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/konst/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/konst/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "### function to help the lemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\") \n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(postag):\n",
    "    if postag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif postag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif postag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif postag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "### lemmatizer   \n",
    "def lemmatize_sentencelist(sentencelist):\n",
    "    lemmatized_sentences = []\n",
    "    for s in sentencelist: \n",
    "        s = s.lower()\n",
    "        pos_s = nltk.pos_tag(s.split())\n",
    "        lemmatized_sentences.append(\" \".join([wnl.lemmatize(w[0], get_wordnet_pos(w[1])) for w in pos_s]))\n",
    "    return lemmatized_sentences\n",
    "\n",
    "df['lemmatized_review'] = df['corrected_review'].apply(lambda x: lemmatize_sentencelist(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown foxes are jumping over the lazy dogs or dog.\n",
      "Lemmatized: the quick brown fox be jump over the lazy dog or dog.\n",
      "---\n",
      "Original: She sells seashells by the seashore.\n",
      "Lemmatized: she sell seashell by the seashore.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "### Testing \n",
    "\n",
    "sentences = [\"The quick brown foxes are jumping over the lazy dogs or dog.\",\n",
    "             \"She sells seashells by the seashore.\"]\n",
    "\n",
    "lemmatized_sentences = lemmatize_sentencelist(sentences)\n",
    "\n",
    "# Print the original and lemmatized sentences\n",
    "for original, lemmatized in zip(sentences, lemmatized_sentences):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The quick brown foxes are jumping over the lazy dogs or dog.\n",
      "Lemmatized: T\n",
      "---\n",
      "Original: She sells seashells by the seashore.\n",
      "Lemmatized: h\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "lema1 = lemmatize_with_postag(sentences[0])\n",
    "\n",
    "for original, lemmatized in zip(sentences, lema1):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox be jump over the lazy dog or dog'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lema1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
